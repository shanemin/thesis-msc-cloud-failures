{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob as glob\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "import dateutil.parser as dparser\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/media/shane/cloud-availability-sacheen-2020-05-11/downdetector' # one directory\n",
    "# root_dir = '/media/shane/cloud-availability-sacheen-2020-05-11/downdetector/2017/201711' # subset of above\n",
    "root_dir = '/media/shane/cloud-availability-sacheen-2020-05-11/downdetector*' # all directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html_data(html_doc):\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    result = []\n",
    "    \n",
    "    timestamp = soup.find('meta', attrs={'name':'generated'})\n",
    "    result.append(timestamp.get('content'))\n",
    "    \n",
    "    # status of the service in last 24hrs (no problems, possible problems, problems)\n",
    "    try:\n",
    "        status = soup.find('div', {'class' : re.compile('alert *')})\n",
    "        result.append(status.get('class')[1])\n",
    "    except:\n",
    "        result.append('')\n",
    "    \n",
    "#     # time since problems started (empty unless problems are ongoing)\n",
    "#     for problems_since in soup.find_all(\"div\", {\"class\": \"event\"}):\n",
    "#         try:\n",
    "#             status = ' '.join(problems_since.text.split())\n",
    "#             date = str(dparser.parse(status, fuzzy=True))\n",
    "#             result.append([date.split()[0].split('-')[1:], date.split()[1]])\n",
    "#         except:\n",
    "#             # some months are apparently out of range\n",
    "#             result.append('')\n",
    "    \n",
    "#     # TECHNICAL DEBT: problems_since is not appended if tag does not exist in the html_doc\n",
    "#     if len(result) != 5:\n",
    "#         result.append('')\n",
    "    \n",
    "#     # most reported problems at this time\n",
    "#     mrp = []\n",
    "#     for most_reported in soup.find_all(\"li\"):\n",
    "#         if '%' in most_reported.text:\n",
    "#             mrp.append(' '.join(most_reported.text.split()))\n",
    "#     result.append(mrp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    source = file.split('/')[4]\n",
    "    archive = ZipFile(file, 'r')\n",
    "    namelist = archive.namelist()\n",
    "    ret = []\n",
    "    for item in namelist:\n",
    "        if 'html' in item:\n",
    "            html = archive.read(item)\n",
    "            service = item.split('/')[1].split('.')[0]\n",
    "            ts = item.split('/')[0]\n",
    "            data = extract_html_data(html)\n",
    "            data.insert(0, ts)\n",
    "            data.insert(0, service)\n",
    "            data.insert(0, source)\n",
    "            ret.append(data)\n",
    "    archive.close()\n",
    "    return(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(root_dir + '/**/*.zip', recursive=True)\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "start_time = time.time()\n",
    "results = pool.map(read_file, [file for file in files])\n",
    "pool.close()\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for outer in results:\n",
    "    for inner in outer:\n",
    "        res.append(inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_names = ['source','service','timestamp_dir','timestamp_site','status']\n",
    "df = pd.DataFrame(res, columns=column_names)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'/home/shane/Documents/thesis/output/parsed/final/downdetector_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
